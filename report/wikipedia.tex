%\documentclass[12pt]{article}

%\usepackage{hyperref}
%\usepackage{graphicx}
%\usepackage{float}

%\begin{document}
\section{Wikipedia}
Written by Quentin Mazars-Simon and Orianne Rollier.

In this section, we will describe each step of the procedure from the Wikipedia data dump to displaying the information on the website.
\begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/wiki-flow.png}
    \caption{Overview of Wikipedia information extraction}
     \label{fig:wikiflow}
\end{figure}
\subsection{Available data}
It is possible to download a dump of all English Wikipedia articles, without edit history, from \href{https://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki}{a Wikipedia page (link)}. We used the dump from February 3rd, 2014, the latest available when we started the project. It consists of a single XML file, which is 10GB compressed, 40GB uncompressed. 
\subsection{Filtering articles}
Obviously, most of Wikipedia articles are irrelevant to our task of find information on weather events that is why we must first filter the dataset. 
\subsubsection{Filtering based on articles' categories}
The easiest way to prune off topic articles, is to look at their category. Unfortunately, categories on Wikipedia are not hierarchically well organized, so we cannot start from a top category such as ``weather event'' and get a list of every relevant subcategories. Indeed, categories may loop or diverge to another subject.\\\\
Knowing this, we decided to filter categories based on a defined set keywords: "blizzard", "cyclone", "hurricane", "typhoon", "derecho", "drought", "nor'easter", "storm", "tornado", "heat wave", "cold wave", "weather event". \\\\
An article belonging to a category which contains one of those keyword is jugged relevant. This allows us to filter out most irrelevant articles, even if there are some false positives (e.g. football players affiliated to a team called ``The Hurricanes'').\\\\
Implementation wise, this filtering consist of a single MapReduce job where the mapper only emits an article if it is relevant, and the reducer combines the articles it receives into a single XML file for each keyword. 
After running the job (usually takes less than 5 minutes), we now only have around 200MB of data, which allows us to do document classification in reasonable time, and filter out the remaining off-topic articles.\\ 
We use Mahout's XMLInputFormat which provides an easy way to split the input into multiple articles delimited by the \texttt{<article>} and \texttt{</article>} tags and send them to the mapper.
\subsubsection{Filtering using document classification}
As already mentioned previously, some articles not related to weather passed through the filtering done with the article's categories. One of the main issue is the presence of a high number of articles related to sport (because of the team names). To overcome this problem, we decided to classify the articles according to predefined classes. \\\\
Before classifying the text was preprocessed mainly to remove citation, references and html entities which are more altering the text than bringing sense to it. \\
For the classification itself, the method used is based on the tfidfs of the articles. The class chosen is the one for which the tfidf vector is the closest to the article's tfidf vector according to the cosine similarity. The classes used are articles related to "wind" (blizzard, cyclone, hurricane, typhoon, derecho, nor'easter, storm, tornado), "hot" weather (drought, heat wave), "cold" weather (cold wave) and sports. The training was done over a sample of 111 articles distributed in the following way: 42 for wind, 20 for rain, 20 for hot, 10 for cold 39 for sports. This uneven distribution is due to a lack of articles in certain categories.\\
The library used here is scikit learn.\\\\
Here are the different steps of the classification:
\begin{enumerate}
  \item Vocabulary construction: This steps consisted in extracting best words to represent the corpus. There are the features used to represent articles in the vector space. To find them we just took the most frequent ones after removing stop words and tokenizing the text. This was all done by the CountVectorizer object of scikit learn. (From this step and the next only approx. 70\% of the samples is used.)
  \item Training: Now that we have a vocabulary to represent articles, using TfidfTransform, the tfidf of each article is computed and fed to the Nearest Centroid Classifier. This classifier when it uses tfidf and cosine similarity is called a Ricchio classifier.
  \item Validation: Having a trained classifer, it can be tested on the remaining 30\% of the samples. After picking randomly 30 samples and classify them we find that one article was misclassified (one from wind as sports). 
  \item Application: As a final step we applied the classifier to the whole set of remaining wikipedia articles around 20 percent were removed. Of course, as already seen in the validation step, articles there are false positive and false negatives. A next step would be to improve the classifier.
\end{enumerate}
\subsection{Extracting information}
Now that we have only relevant articles, we can start extracting information. \\
The general idea is to first locate the article's infobox which, if it exists, usually provides detailed information on date and location of the event (date formed, date dissipated, areas). If there is no infobox, or we still need more information, we try to extract it from the article's title, and finally, as a last resort we try to find the information in article's body.\\\\
We only use a mapper to extract information, the reducer is only used to combine the results.
After extraction, we have a single CSV file (separated by tabs) containing the following columns: title, category, start date, end date, location. At this point we have less than a megabyte of data. Running this job usually takes less than two minutes.
\subsubsection{Extracting dates}
To extract dates we use multiple regexes, to cover the range of available date formats (DD/MM/YYYY; YYYY$|$MM$|$DD; m dd, YYYY; dd m YYYY; etc.). We then convert them all to a predefined date format: DD/MM/YYYY. 
\subsubsection{Extracting locations}
To extract location, we use a regex based on the lists of continents, countries and american states as well as their corresponding demonyms given in \href{http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names}{this Wikipedia article}.
\subsection{Displaying information}
In order to display correctly the information, we first run a small python that creates http link for each article's title (\texttt{<a href='http://en.wikipedia.org/wiki/\$TITLE'> \$TITLE </a>}), and get the number of references of an articles from Wikipedia using a GET request (\texttt{https://en.wikipedia.org/wiki/Special:WhatLinksHere/\$TITLE}).\\
We then sort the articles by their number of reference, to display most important events first\\\\
On the webpage, we display the information as an HTML table whose rows can be easily filtered to show only relevant events.
\subsection{Results}
The quality of the result greatly depends on the article's category. For example, each hurricane usually has its own article which almost always contains an infobox with precise start/end date as well as areas affected, whereas tornadoes are usually grouped under the same article where extracting information is harder (e.g. tornadoes of  2012); also for some weather events (such as cold/heat waves) it is hard to define clear start/end dates.
%\end{document}
